{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtDeJtzJH36u+J7P3oB8Qx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"90_9hbq2M8qq"},"source":["#**Basic task in Natural Language Processing(NLP):**"]},{"cell_type":"code","source":["In NLP and text Mining we do preprocessing of text data. where we apply several operation text data. Most Commanly used operation are given below. \n","1. Tokenization.\n","2. Part of Speech tagging.\n","3. Stop words recognition.\n","4. Entity Name recognition.\n","5. Lemmatization and Stemming.\n","6. Dependency Parsing.\n","7. Word similarity."],"metadata":{"id":"Mlj3GuKeGEHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1oVA3af6lWHH"},"source":["#**1. Tokenization:**"]},{"cell_type":"markdown","source":["* In this step, the text is split into smaller units.\n","* We can use either sentence tokenization or word tokenization based on our problem statement."],"metadata":{"id":"3CSmY9EiGQFf"}},{"cell_type":"code","metadata":{"id":"knbU0TumNaNU"},"source":["# Import Spacy\n","import spacy as sp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhLRag2eNjEK"},"source":["# Load the english library\n","import en_core_web_sm as en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4PDWZQRNoDu"},"source":["# create nlp object and Load the english module of spacy library into it.\n","nlp = sp.load('en_core_web_sm')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QxBgkJ8N68C"},"source":["**Method-1: Using Spacy**"]},{"cell_type":"code","metadata":{"id":"j9ZNqnEeM9hR"},"source":["text = \"I am learning natural language processing. The course is offered by univercity.It is in Mumbai. Ph.d Sunil is Instructor\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akkAN9dbNWuI"},"source":["# Note: To work with spacy library we need to associate above string with nlp object\n","doc = nlp(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kISjqLvgNzZw","executionInfo":{"status":"ok","timestamp":1639455582141,"user_tz":-330,"elapsed":638,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c5922fd9-b7b3-4874-f7f2-58a4d0c58498"},"source":["# Tokenization using word\n","for x in doc:\n","  print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I\n","am\n","learning\n","natural\n","language\n","processing\n",".\n","The\n","course\n","is\n","offered\n","by\n","univercity\n",".\n","It\n","is\n","in\n","Mumbai\n",".\n","Ph.d\n","Sunil\n","is\n","Instructor\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqipMBA9OKgw","executionInfo":{"status":"ok","timestamp":1639455587638,"user_tz":-330,"elapsed":537,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"9bd127a6-abc5-4e1e-eddf-787d710645c8"},"source":["# Tokenization using sentence\n","text = 'I am learning natural language processing. The course is offered by univercity.It is in Mumbai. Ph.d Sunil is Instructor'\n","text.split('.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I am learning natural language processing',\n"," ' The course is offered by univercity',\n"," 'It is in Mumbai',\n"," ' Ph',\n"," 'd Sunil is Instructor']"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"eTdU0DIkOO28"},"source":["**Method-2: Tokenization using Using NLTK**"]},{"cell_type":"code","metadata":{"id":"GLDSrAkHOQGj"},"source":["# Install nltk libary and its submodule\n","import nltk\n","import nltk.tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpkjGv4ROTrL","executionInfo":{"status":"ok","timestamp":1639455509525,"user_tz":-330,"elapsed":547,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"499f3d5b-6484-496c-cbff-efc382981404"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**Note:** Without dawnloading this package if you will do word tokenization then it will give error and will recommend you to dawnload 'punk'"],"metadata":{"id":"o01niwLqdATm"}},{"cell_type":"code","metadata":{"id":"Tgw3ZoqWOYMx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639455521688,"user_tz":-330,"elapsed":530,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"f1820193-3f21-402c-e94b-0367f75227be"},"source":["# Word toenization\n","text = \"I am learning natural language processing. The course offered by univercity.It is in Mumbai. Ph.d Sunil is Instructor\"\n","word_tokenize(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I',\n"," 'am',\n"," 'learning',\n"," 'natural',\n"," 'language',\n"," 'processing',\n"," '.',\n"," 'The',\n"," 'course',\n"," 'offered',\n"," 'by',\n"," 'univercity.It',\n"," 'is',\n"," 'in',\n"," 'Mumbai',\n"," '.',\n"," 'Ph.d',\n"," 'Sunil',\n"," 'is',\n"," 'Instructor']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vtqP93EkObHD","executionInfo":{"status":"ok","timestamp":1639455531666,"user_tz":-330,"elapsed":541,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"cbc38490-5e79-475d-96b4-d2dbfd4bdc88"},"source":["# Sentence Tokenization\n","text = \"I am learning natural language processing. The course offered by univercity. It is in Mumbai. Ph.d Sunil is Instructor\"\n","sent_tokenize(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I am learning natural language processing.',\n"," 'The course offered by univercity.',\n"," 'It is in Mumbai.',\n"," 'Ph.d Sunil is Instructor']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"xN-xnpMbSewl"},"source":["# **2. Part of Speech**"]},{"cell_type":"markdown","source":["- In this module here we will identify each one of the word of sentences.\n","- POS (part-of-speech): Noun, Adjective, Verb, Aux verb, Determiner, Punctuation Adpostion etc.\n","- For More details visit: https://ashutoshtripathi.com/2020/04/13/parts-of-speech-tagging-and-dependency-parsing-using-spacy-nlp/"],"metadata":{"id":"FS9QayPUGXV2"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Iwu7ixQSlRq","executionInfo":{"status":"ok","timestamp":1635062177582,"user_tz":-330,"elapsed":485,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"6040913d-c92e-420c-b880-db27bf3bc8bd"},"source":["# Identify each token of doc sentence\n","for x in doc:\n","  print(x.text,\"--\", x.pos_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I -- PRON\n","am -- AUX\n","learning -- VERB\n","natural -- ADJ\n","language -- NOUN\n","processing -- NOUN\n",". -- PUNCT\n","The -- DET\n","course -- NOUN\n","is -- AUX\n","offered -- VERB\n","by -- ADP\n","univercity -- NOUN\n",". -- PUNCT\n","It -- PRON\n","is -- AUX\n","in -- ADP\n","Mumbai -- PROPN\n",". -- PUNCT\n","Ph.d -- PROPN\n","Sunil -- PROPN\n","is -- AUX\n","Instructor -- NOUN\n"]}]},{"cell_type":"markdown","metadata":{"id":"S-GXW6jrTZnG"},"source":["# **3. Stop Words**"]},{"cell_type":"markdown","metadata":{"id":"m56Fvw1xTpqj"},"source":["* Stop words are the Words that appear very often and don't help to understand the context of the document.\n","* Stopwords are the commonly used words and are removed from the text as they do not add any value to the analysis. \n","* These words carry less information and have no meaning."]},{"cell_type":"code","metadata":{"id":"GAWyNrTsTrBF"},"source":["from spacy.lang.en.stop_words import STOP_WORDS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8R2gaoW4T8K-","executionInfo":{"status":"ok","timestamp":1635062469738,"user_tz":-330,"elapsed":23,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"a8d6c263-6c6c-4e04-87e0-8b9b79a4f1a8"},"source":["print(STOP_WORDS)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'after', 'the', 'n‘t', 'never', 'into', 'than', 'twenty', 'on', 'front', 'about', 'anyone', 'keep', 'his', 'at', 'third', 'her', 'then', 'serious', 'where', 'else', 'yourselves', 'done', 'latter', 'eleven', 'wherein', \"'ll\", 'i', 'seems', 'would', 'former', 'be', 'go', '‘s', 'full', 'no', 'last', 'therein', 'for', 'some', 'around', 'just', '’s', 'mine', 'or', 'made', 'fifty', 'otherwise', 'every', 'throughout', 'whereafter', 'among', 'me', 'rather', 'enough', 'only', 'whom', 'alone', 'something', 'really', 'noone', 'whether', 'sometime', '’m', 'these', 'any', 'next', 'us', 'put', 'less', 'most', 'thereby', 'have', 'except', 'too', 'always', 'first', 'herein', 'what', 'ourselves', 'few', 'becomes', 'but', 'until', 'our', 'towards', \"'re\", 'besides', 'several', 'hereupon', 'top', 'anything', 'their', 'whence', 'nowhere', 'others', 'had', 'both', 'still', 'also', 'quite', 'see', 'many', 'either', \"'ve\", 'who', 'why', '‘ll', 'whoever', 'show', 'though', 'formerly', 'seemed', \"'m\", 'own', '‘d', 'indeed', 'take', 'n’t', 'whereby', 'using', '‘m', 'becoming', 'against', 'those', 'you', 'onto', 'empty', 'from', 'here', 'did', 'itself', 'even', 'which', 'up', 'during', 'least', 'more', 'beyond', 'were', 'perhaps', 'a', 'if', 'was', \"'s\", 'nobody', 'hence', 'should', 'am', 'seeming', 'been', 'him', 'anyhow', 'ca', 'beside', 're', 'although', 'are', 'due', 'might', 'sixty', 'between', 'therefore', 'call', 'nine', 'name', 'could', 'per', 'nor', 'not', 'somehow', 'latterly', 'thru', '‘ve', 'eight', 'well', 'above', 'moreover', 'hundred', 'however', 'of', 'off', 'under', 'none', 'ever', 'whereas', 'is', 'side', 'namely', 'almost', 'before', 'across', 'this', 'how', 'below', 'while', 'they', 'forty', 'your', 'as', 'please', 'them', 'yourself', 'twelve', 'whenever', 'such', '’d', 'nevertheless', 'used', 'another', 'and', 'by', 'thus', 'there', 'seem', 'move', 'together', 'elsewhere', 'somewhere', 'themselves', 'became', 'say', 'has', 'that', 'does', 'behind', '‘re', 'various', '’ll', 'will', 'unless', 'with', 'amongst', 'toward', 'do', 'doing', 'anywhere', 'whole', 'ours', 'whereupon', 'everywhere', 'when', 'whither', '’ve', 'myself', 'she', 'can', 'fifteen', 'very', 'same', 'all', 'back', 'via', 'its', 'through', 'part', 'being', 'one', 'it', 'again', \"n't\", 'become', 'hereafter', 'mostly', 'four', 'already', 'two', 'thereupon', 'thereafter', 'hers', 'get', 'hereby', 'everything', 'wherever', 'anyway', 'sometimes', 'to', 'beforehand', 'further', 'yours', 'meanwhile', 'five', 'he', 'without', 'whose', 'within', 'ten', 'someone', 'give', 'three', 'so', 'in', 'thence', 'cannot', 'my', 'himself', 'out', 'other', 'along', 'must', 'since', 'much', 'afterwards', 'each', 'regarding', 'often', '’re', 'yet', 'upon', 'nothing', 'down', 'we', 'six', \"'d\", 'make', 'once', 'bottom', 'may', 'an', 'whatever', 'amount', 'now', 'because', 'neither', 'everyone', 'herself', 'over'}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIAGDwrVT_yD","executionInfo":{"status":"ok","timestamp":1635062477597,"user_tz":-330,"elapsed":5,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c6478cec-e362-4d72-f4f2-8e228e8effbf"},"source":["len(STOP_WORDS)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["326"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htAOHBTvUChz","executionInfo":{"status":"ok","timestamp":1635062480274,"user_tz":-330,"elapsed":471,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"3de66cc7-3425-40aa-cc13-4066e8668c7b"},"source":["'it' in STOP_WORDS"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ze_yuxJUFPH","executionInfo":{"status":"ok","timestamp":1635062483113,"user_tz":-330,"elapsed":11,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"6bd8185e-1b78-4dd6-cf7f-f1c2b55ca37f"},"source":["nlp.vocab['it'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKxxbNh0UJj8","executionInfo":{"status":"ok","timestamp":1635062485813,"user_tz":-330,"elapsed":17,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"1c0be09c-f07d-422f-a7c6-6fc94a7ebb18"},"source":["nlp.vocab['walk'].is_stop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofSLx6zxUMNk","executionInfo":{"status":"ok","timestamp":1635062498779,"user_tz":-330,"elapsed":487,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"d2724920-aba7-4821-ee6b-a2b5995cd647"},"source":["for x in doc:\n","  if nlp.vocab[x.text].is_stop:\n","    print(x.text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I\n","am\n","The\n","is\n","by\n","It\n","is\n","in\n","is\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQIXFv83U2vp","executionInfo":{"status":"ok","timestamp":1635062618847,"user_tz":-330,"elapsed":451,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"8ba9360b-ebca-4cf1-ebb1-3a40aa63867b"},"source":["for x in doc:\n","  if not nlp.vocab[x.text].is_stop:\n","    print(x.text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["learning\n","natural\n","language\n","processing\n",".\n","course\n","offered\n","univercity\n",".\n","Mumbai\n",".\n","Ph.d\n","Sunil\n","Instructor\n"]}]},{"cell_type":"markdown","metadata":{"id":"mNLAdorWVI2U"},"source":["# **4. Named-Entity Recognition (NER)**"]},{"cell_type":"markdown","source":["* Named entity recognition is the first step towards Information Retrieval.\n","* It is the technique to extract important and useful information for the unstructured raw text document.\n","* Named Entity Recognition NER works by locating and identifying the named entities present in unstructured text into the standard categories such as person names, locations, organizations, time expressions, quantities, monetary values, percentage, codes etc."],"metadata":{"id":"tNnlByidGhRN"}},{"cell_type":"code","metadata":{"id":"RYcrabPFV2qO"},"source":["text = 'IBM is a US company on information technology. It is located in San Francisco and revenue in 2018 was approximately 320 billion dolars'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuwsvNDbWLKo"},"source":["doc2 = nlp(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slGJWbJOWOgX","executionInfo":{"status":"ok","timestamp":1639456122716,"user_tz":-330,"elapsed":10,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"4702aa2a-afd6-4424-91a3-6f09e3e0d342"},"source":["for x in doc2.ents:\n","  print(x,\"--\", x.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IBM -- ORG\n","US -- GPE\n","San Francisco -- GPE\n","2018 -- DATE\n","approximately 320 billion -- CARDINAL\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCjfKwUfWaDM","executionInfo":{"status":"ok","timestamp":1635063026329,"user_tz":-330,"elapsed":461,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"1fe35b75-6dd4-48d0-964d-1c55d117069d"},"source":["for x in doc.ents:\n","  print(x,\"--\", x.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mumbai -- GPE\n"]}]},{"cell_type":"code","metadata":{"id":"D56hdyd9Wlmq"},"source":["from spacy import displacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"bBm4ZlXuWnDa","executionInfo":{"status":"ok","timestamp":1635063081716,"user_tz":-330,"elapsed":458,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"fe107512-cf88-402a-ba62-a271925e6087"},"source":["displacy.render(doc2, style = 'ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    IBM\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," is a \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    US\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," company on information technology. It is located in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    San Francisco\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," and revenue in \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2018\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," was \n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    approximately 320 billion\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n"," dolars</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"F0FT9CI2WrKO"},"source":["text = 'Bill Gates was born in Seattle on 1955-10-28 and is the founder of Microsoft'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r3LD1u_IWuVe"},"source":["doc3 = nlp(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSZ4h2LFWx_i","executionInfo":{"status":"ok","timestamp":1635063157824,"user_tz":-330,"elapsed":462,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"82eecafe-42ee-4f00-ddf0-a7abec67db71"},"source":["for x in doc3.ents:\n","  print(x.text, x.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bill Gates PERSON\n","Seattle GPE\n","1955-10-28 DATE\n","Microsoft ORG\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"yYWinZecW0rt","executionInfo":{"status":"ok","timestamp":1635063160054,"user_tz":-330,"elapsed":31,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c370c9bb-af62-435c-9113-95e5b2acab52"},"source":["displacy.render(doc3, style = 'ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Bill Gates\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n"," was born in \n","<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Seattle\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n","</mark>\n"," on \n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    1955-10-28\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n"," and is the founder of \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Microsoft\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n","</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoZzq3xsW3XG","executionInfo":{"status":"ok","timestamp":1635063192431,"user_tz":-330,"elapsed":513,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c799be75-6f0f-402b-d994-c48c2fb9bd62"},"source":["for x in doc3.ents:\n","  if x.label_ == 'PERSON':\n","    print(x.text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bill Gates\n"]}]},{"cell_type":"markdown","metadata":{"id":"nBazl84XYTjh"},"source":["# **5. Lemmatization and Stemming**"]},{"cell_type":"markdown","source":["* The aim of both processes is the same: reducing the inflectional forms of each word into a common base or root.\n","* Both process are different, let’s see what is stemming and lemmatization.\n","* Stemming usually refers to a crude process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational units (the obtained element is known as the stem).\n","* On the other hand, lemmatization consists in doing things properly with the use of a vocabulary and morphological analysis of words, to return the base or dictionary form of a word, which is known as the lemma.\n","* If we stem the sentence “I saw an amazing thing ”we would obtain ‘s’ instead of ‘saw’, but if we lemmatize it we would obtain ‘see’, which is the lemma.\n","  ![image.png](data:image/png;base64,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)\n","* Both techniques could remove important information but also help us to normalize our corpus (although lemmatization is the one that is usually applied.Actually stemming create some words, that may not have any meaning, so we usually use lemmatization.\n","* The aim of both processes is the same: reducing the inflectional forms of each word into a common base or root.\n","* Both process are different, let’s see what is stemming and lemmatization.\n","* Stemming usually refers to a crude process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational units (the obtained element is known as the stem).\n","* On the other hand, lemmatization consists in doing things properly with the use of a vocabulary and morphological analysis of words, to return the base or dictionary form of a word, which is known as the lemma.\n","* If we stem the sentence “I saw an amazing thing ”we would obtain ‘s’ instead of ‘saw’, but if we lemmatize it we would obtain ‘see’,which is the lemma.\n"],"metadata":{"id":"Cm-20pGpGxmH"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lmpe0MpuZEJM","executionInfo":{"status":"ok","timestamp":1635064159751,"user_tz":-330,"elapsed":493,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"f7b5de7a-7c41-40de-b522-fd0a78fce8c0"},"source":["# Find the lemma of each word\n","for x in doc:\n","  print(x.text,\"--\", x.lemma_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I -- -PRON-\n","am -- be\n","learning -- learn\n","natural -- natural\n","language -- language\n","processing -- processing\n",". -- .\n","The -- the\n","course -- course\n","is -- be\n","offered -- offer\n","by -- by\n","univercity -- univercity\n",". -- .\n","It -- -PRON-\n","is -- be\n","in -- in\n","Mumbai -- Mumbai\n",". -- .\n","Ph.d -- Ph.d\n","Sunil -- Sunil\n","is -- be\n","Instructor -- instructor\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gb5snG_Sa8td","executionInfo":{"status":"ok","timestamp":1635064261806,"user_tz":-330,"elapsed":465,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"ff5c2682-edf3-45c0-f067-b91d811a7913"},"source":["docs = nlp('learn learning watch watching watched')\n","[x.lemma_ for x in docs]                                        # List Comprehension"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['learn', 'learn', 'watch', 'watch', 'watch']"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"CDM_LT8NbZjJ"},"source":["**Lemmatization vs Stemming**"]},{"cell_type":"code","metadata":{"id":"NZCsV9enbSx4"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DDNrkOkbdwe"},"source":["stemmer = nltk.stem.PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"umU9XyPfbnUb","executionInfo":{"status":"ok","timestamp":1635064383503,"user_tz":-330,"elapsed":478,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"236af815-bd8a-4f5d-aa93-83edc56528c5"},"source":["stemmer.stem('learning')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'learn'"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KGU9ns54bpKM","executionInfo":{"status":"ok","timestamp":1635064400080,"user_tz":-330,"elapsed":447,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c99f41b8-9854-4cee-958c-9e4734657037"},"source":["stemmer.stem('watching')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'watch'"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H70O1JdVbyeb","executionInfo":{"status":"ok","timestamp":1635064444126,"user_tz":-330,"elapsed":461,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"0c442418-f318-4948-ec2b-37a840272d98"},"source":["for x in docs:\n","  print(x.text, x.lemma_, stemmer.stem(x.text))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["learn learn learn\n","learning learn learn\n","watch watch watch\n","watching watch watch\n","watched watch watch\n"]}]},{"cell_type":"code","metadata":{"id":"fx8Ck-_fcKDW"},"source":["docs1 = nlp('I Saw an amazing thing')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWiEaDLdccR-","executionInfo":{"status":"ok","timestamp":1635064706977,"user_tz":-330,"elapsed":469,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"7e0c59a9-42fb-44c6-ddb4-2f2989e83532"},"source":["for x in docs1:\n","  print(x,\"--\", x.lemma_,\"--\", stemmer.stem(x.text))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I -- -PRON- -- I\n","Saw -- see -- saw\n","an -- an -- an\n","amazing -- amazing -- amaz\n","thing -- thing -- thing\n"]}]},{"cell_type":"markdown","metadata":{"id":"9i6OcKZJdGgc"},"source":["#**6. Dependency Parsing**"]},{"cell_type":"markdown","source":["* Dependency parsing is the process of extracting the dependencies of a sentence to represent its grammatical structure.\n","* It defines the dependency relationship between headwords and their dependents.\n","* The head of a sentence has no dependency and is called the root of the sentence.\n","*The verb is usually the head of the sentence. All other words are linked to the headword.\n","* It Shows parent Child relationship (Parent is head of the word while Child is dependent word).\n","* The dependencies can be mapped in a directed graph representation:\n","* Words are the nodes.\n","* The grammatical relationships are the edges.\n","* Dependency parsing helps you know what role a word plays in the text and how different words relate to each other.\n","* It’s also used in shallow parsing and named entity recognition."],"metadata":{"id":"wiiyCeb_Hlp3"}},{"cell_type":"markdown","metadata":{"id":"SWbbgJlJhs_q"},"source":["**Example 1**"]},{"cell_type":"code","metadata":{"id":"54MY27pgdgmw"},"source":["document = nlp('book a ticket from London to Paris')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jqW6G2KOhw99","executionInfo":{"status":"ok","timestamp":1639457086071,"user_tz":-330,"elapsed":528,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"c0d3b16e-a117-4dd0-d97b-e7d511e7a89e"},"source":["origin = document[4]\n","destiny = document[6]\n","print(origin, destiny)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["London Paris\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfgQ1ENhh07R","executionInfo":{"status":"ok","timestamp":1639457089321,"user_tz":-330,"elapsed":7,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"547952da-534e-4a49-efd2-b136bcd00205"},"source":["list(origin.ancestors)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[from, ticket, book]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Te9pg18sh4ys","executionInfo":{"status":"ok","timestamp":1639457145409,"user_tz":-330,"elapsed":517,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"d29dfec6-77f6-4e6c-8185-40d193ba4c5d"},"source":["list(destiny.ancestors)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[to, ticket, book]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISHHtXrOh8es","executionInfo":{"status":"ok","timestamp":1639457148427,"user_tz":-330,"elapsed":537,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"ef922bbc-62e1-48ea-fcbe-faad47a49793"},"source":["document[0].is_ancestor(document[2])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"bSXcHs-aiMlG"},"source":["**Example 2**"]},{"cell_type":"code","metadata":{"id":"C2UQxoSiiJCi"},"source":["document = nlp('Book a table for the restaurant and a taxi to the hotel')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0v4nbxfsiJEc"},"source":["tasks = document[2], document[8]\n","locations = document[5], document[11]\n","print(tasks, locations)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WTG46TYiT5S"},"source":["for x in locations:\n","  print('-----', x)\n","  for y in x.ancestors:\n","    print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLmodjHviW2L"},"source":["for x in locations:\n","  for y in x.ancestors:\n","    if y in tasks:\n","      print('Reservation of a {} to the {}'.format(y, x))\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4zFWMBIiaZ-"},"source":["list(document[5].children)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaohah4-id8f"},"source":["**Example 3**"]},{"cell_type":"code","metadata":{"id":"ECZX3nX3idNU"},"source":["from spacy import displacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93KqfxzCiksA"},"source":["document = nlp('Book a table for the restaurant and a taxi to the hotel')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_elLNPwinBj"},"source":["displacy.render(document, style='dep', jupyter=True, options={'distance': 90})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5YTIlsTisoD"},"source":["list(document[2].ancestors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlYwJIH-itfO"},"source":["list(document[2].children)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tT3aNGGuiw6U"},"source":["**Example 4**"]},{"cell_type":"code","metadata":{"id":"zZbib_sMi0Hr"},"source":["document = nlp('What places can we visit in London and stay in Paris?')\n","locations = document[6], document[10]\n","actions = document[4], document[8]\n","print(locations, actions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFmsUja2i25U"},"source":["for local in locations:\n","  #print(local)\n","  for action in local.ancestors:\n","    if action in actions:\n","      print('{} to {}'.format(local, action))\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3kL3Z-Ii6jR"},"source":["displacy.render(document, style='dep', jupyter=True, options={'distance': 90})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkDpn7-2jCdi"},"source":["#**7. Similarity between words and sentences**"]},{"cell_type":"markdown","metadata":{"id":"SmsCyTZlj2Y1"},"source":["*   spaCy uses the GloVe algorithm (Global Vectors for Word Representation)\n","*   Original paper: https://nlp.stanford.edu/pubs/glove.pdf"]},{"cell_type":"markdown","metadata":{"id":"w2XErZs5kwfr"},"source":["**Example-1:**"]},{"cell_type":"code","metadata":{"id":"-nEf_XxyjJQf"},"source":["w1 = nlp('hello')\n","w2 = nlp('hi')\n","w3 = nlp('or')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCO7cVTij_se","executionInfo":{"status":"ok","timestamp":1635066735512,"user_tz":-330,"elapsed":17,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"e1df3ad7-02df-4730-b07c-1e7d36114434"},"source":["w1.similarity(w2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7182743634819724"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ek8ltw0ikCKU","executionInfo":{"status":"ok","timestamp":1635066739225,"user_tz":-330,"elapsed":13,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"9c6d5a2a-82fd-422f-8e5a-14edd2d13b25"},"source":["w2.similarity(w1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7182743634819724"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20IxjykNkEo2","executionInfo":{"status":"ok","timestamp":1635066741892,"user_tz":-330,"elapsed":10,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"62048eda-6377-4128-ed11-ed07c9bf1d46"},"source":["w1.similarity(w3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.05859708943069697"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76-1nHmFkI2y","executionInfo":{"status":"ok","timestamp":1635066744572,"user_tz":-330,"elapsed":8,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"742de200-90c1-4cc3-a18b-3e04d36b98ab"},"source":["w2.similarity(w3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.2999657546805652"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"n3grJkJWk2tg"},"source":["**Example-2**"]},{"cell_type":"code","metadata":{"id":"xR6gzlONkLO4"},"source":["text1 = nlp('When will the new movie be released?')\n","text2 = nlp('The new movie will be released next month')\n","text3 = nlp('What color is the car?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l85RSkNXkPoc","executionInfo":{"status":"ok","timestamp":1635066766844,"user_tz":-330,"elapsed":5437,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"cd9e8940-7629-4a20-eda5-b7be7072de1d"},"source":["text1.similarity(text2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7851327039186423"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3b_NSg-kSPy","executionInfo":{"status":"ok","timestamp":1635066769942,"user_tz":-330,"elapsed":464,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"ae09cf9f-d10a-4b4b-cc16-f6827841b51c"},"source":["text1.similarity(text3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.38287079848766864"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"GFm6qbVkkWs9"},"source":["**Example-3**"]},{"cell_type":"code","metadata":{"id":"PISTB5xwkZwB"},"source":["text = nlp('cat dog horse person')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3PTeyrCkc_L","executionInfo":{"status":"ok","timestamp":1635066854562,"user_tz":-330,"elapsed":12,"user":{"displayName":"SUNIL YADAV","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GganiM423H1NKko6sAm8eUAcIJE-WpNykz0NN8w=s64","userId":"05365530080582545014"}},"outputId":"e87bd69c-2954-40fe-ce64-49c86a216ff2"},"source":["for text1 in text:\n","  print('----', text1)\n","  for text2 in text:\n","    print(text2)\n","    similarity = text1.similarity(text2) * 100\n","    print('{} is {}% similar to {}'.format(text1, similarity, text2))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---- cat\n","cat\n","cat is 100.0% similar to cat\n","dog\n","cat is 46.62756025791168% similar to dog\n","horse\n","cat is 42.55187213420868% similar to horse\n","person\n","cat is 22.016122937202454% similar to person\n","---- dog\n","cat\n","dog is 46.62756025791168% similar to cat\n","dog\n","dog is 100.0% similar to dog\n","horse\n","dog is 64.57331776618958% similar to horse\n","person\n","dog is 41.10945165157318% similar to person\n","---- horse\n","cat\n","horse is 42.55187213420868% similar to cat\n","dog\n","horse is 64.57331776618958% similar to dog\n","horse\n","horse is 100.0% similar to horse\n","person\n","horse is 56.20217323303223% similar to person\n","---- person\n","cat\n","person is 22.016122937202454% similar to cat\n","dog\n","person is 41.10945165157318% similar to dog\n","horse\n","person is 56.20217323303223% similar to horse\n","person\n","person is 100.0% similar to person\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  \"__main__\", mod_spec)\n"]}]}]}